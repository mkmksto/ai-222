{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c38bfd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035eaf4e",
   "metadata": {},
   "source": [
    "## nn.linear\n",
    "\n",
    "equivalent to y = XW + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_layer.weight: Parameter containing:\n",
      "tensor([[0.3240]], requires_grad=True)\n",
      "linear_layer.bias: Parameter containing:\n",
      "tensor([0.3829], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "D_in = 1\n",
    "D_out = 1\n",
    "\n",
    "# notice that what is printed is exactly the same as the manual implementation\n",
    "linear_layer = torch.nn.Linear(D_in, D_out)\n",
    "print(f\"linear_layer.weight: {linear_layer.weight}\")\n",
    "print(f\"linear_layer.bias: {linear_layer.bias}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec9a06",
   "metadata": {},
   "source": [
    "ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5780b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: tensor([-1.0000,  0.0000,  1.0000, -2.0000, -0.5000,  3.0000])\n",
      "activated_data: tensor([0., 0., 1., 0., 0., 3.])\n"
     ]
    }
   ],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "sample_data = torch.tensor([-1.0, 0.0, 1.0, -2.0, -0.5, 3.0])\n",
    "activated_data = relu(sample_data)\n",
    "\n",
    "print(f\"original data: {sample_data}\")\n",
    "print(f\"activated_data: {activated_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe47ac6",
   "metadata": {},
   "source": [
    "# softmax\n",
    "\n",
    "Note: since softmax isn't reductive like torch.mean, torch.sum, etc. doing something like dim=0, softmax would operate across the first dimension, in this case the rows, which is equivalent to going vertically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb76b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities_dim_0:\n",
      "tensor([[0.8808, 0.9820, 0.9975, 0.2689],\n",
      "        [0.1192, 0.0180, 0.0025, 0.7311]])\n",
      "probabilities_dim_1:\n",
      "tensor([[0.0883, 0.2399, 0.6521, 0.0197],\n",
      "        [0.1671, 0.0615, 0.0226, 0.7488]])\n"
     ]
    }
   ],
   "source": [
    "softmax_dim_0 = torch.nn.Softmax(dim=0)\n",
    "softmax_dim_1 = torch.nn.Softmax(dim=-1)  # -1 = last dimension\n",
    "\n",
    "logits = torch.tensor([[1.0, 2.0, 3.0, -0.5], [-1.0, -2.0, -3.0, 0.5]])\n",
    "probabilities_dim_0 = softmax_dim_0(logits)\n",
    "probabilities_dim_1 = softmax_dim_1(logits)\n",
    "\n",
    "print(f\"probabilities_dim_0:\\n{probabilities_dim_0}\")\n",
    "print(f\"probabilities_dim_1:\\n{probabilities_dim_1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bff980",
   "metadata": {},
   "source": [
    "# nn.layer_norm\n",
    "\n",
    "prevents exploding/vanishing gradients by rescaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ee9e3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_features:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "normalized_features:\n",
      "tensor([[-1.2247,  0.0000,  1.2247],\n",
      "        [-1.2247,  0.0000,  1.2247]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "norm_layer = torch.nn.LayerNorm(\n",
    "    normalized_shape=3\n",
    ")  # our word vectors have a feature dimension of 3\n",
    "input_features = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "normalized_features = norm_layer(input_features)\n",
    "\n",
    "print(f\"input_features:\\n{input_features}\")\n",
    "print(f\"normalized_features:\\n{normalized_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874dea1",
   "metadata": {},
   "source": [
    "# nn.dropout\n",
    "\n",
    "prevents overfitting by randomly dropping out some neurons (zeroing out some neurons)\n",
    "\n",
    "Note that dropout is applied during training, not testing nor inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9254552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_during_train:\n",
      "tensor([[2., 2., 2., 0., 0., 0., 0., 2., 2., 0.]])\n",
      "output_during_test:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "dropout_layer = torch.nn.Dropout(p=0.5)  # zero out 50% of the neurons\n",
    "input_tensor = torch.ones(1, 10)\n",
    "\n",
    "# activate dropout for training\n",
    "dropout_layer.train()\n",
    "output_during_train = dropout_layer(input_tensor)\n",
    "\n",
    "# deactivate dropout for testing\n",
    "# note that in eval mode, it's just an identity function since it just returns the input\n",
    "dropout_layer.eval()\n",
    "output_during_test = dropout_layer(input_tensor)\n",
    "\n",
    "print(f\"output_during_train:\\n{output_during_train}\")\n",
    "print(f\"output_during_test:\\n{output_during_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3af2f",
   "metadata": {},
   "source": [
    "# Simple Linear Regression using nn.Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d481e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- model architecture: \n",
      "LinearRegression(\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "---- model params: \n",
      "linear.weight: tensor([[0.7645]])\n",
      "linear.bias: tensor([0.8300])\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.linear = torch.nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "lr_model = LinearRegression(1, 1)\n",
    "print(\"---- model architecture: \")\n",
    "print(lr_model)\n",
    "print(\"---- model params: \")\n",
    "for name, param in lr_model.named_parameters():\n",
    "    print(f\"{name}: {param.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b79bfc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[:3]: tensor([[0.4705],\n",
      "        [1.6563],\n",
      "        [0.5153]])\n",
      "X.shape: torch.Size([10, 1])\n",
      "W_true: tensor([[2.]])\n",
      "b_true: 1.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(30)\n",
    "X = torch.randn(10, 1)\n",
    "print(f\"X[:3]: {X[:3]}\")\n",
    "\n",
    "W_true = torch.tensor([[2.0]])\n",
    "b_true = torch.tensor(1.0)\n",
    "print(f\"X.shape: {X.shape}\")\n",
    "print(f\"W_true: {W_true}\")\n",
    "print(f\"b_true: {b_true}\")\n",
    "\n",
    "y_true = X @ W_true + b_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ded64c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 6.656859568465734e-06\n",
      "lr_model.linear.weight: 1.951899766921997\n",
      "lr_model.linear.bias: 0.9509152770042419\n",
      "step: 20, loss: 0.000408816005801782\n",
      "lr_model.linear.weight: 1.985743761062622\n",
      "lr_model.linear.bias: 0.9859563112258911\n",
      "step: 40, loss: 6.365532499330584e-06\n",
      "lr_model.linear.weight: 1.9979428052902222\n",
      "lr_model.linear.bias: 0.9989743232727051\n",
      "step: 60, loss: 1.0216799637419172e-06\n",
      "lr_model.linear.weight: 1.999178409576416\n",
      "lr_model.linear.bias: 0.999602198600769\n",
      "step: 80, loss: 7.304174118871742e-07\n",
      "lr_model.linear.weight: 1.9992291927337646\n",
      "lr_model.linear.bias: 0.9994306564331055\n",
      "W_true: tensor([[2.]])\n",
      "b_true: 1.0\n",
      "----------------\n",
      "lr_model.linear.weight: Parameter containing:\n",
      "tensor([[1.9999]], requires_grad=True)\n",
      "lr_model.linear.bias: Parameter containing:\n",
      "tensor([0.9999], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "epochs, lr = 100, 0.05\n",
    "\n",
    "optimizer = optim.Adam(lr_model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_hat = lr_model(X)\n",
    "\n",
    "    loss = loss_fn(y_hat, y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"step: {epoch}, loss: {loss.item()}\")\n",
    "        print(f\"lr_model.linear.weight: {lr_model.linear.weight.item()}\")\n",
    "        print(f\"lr_model.linear.bias: {lr_model.linear.bias.item()}\")\n",
    "\n",
    "print(f\"W_true: {W_true}\")\n",
    "print(f\"b_true: {b_true}\")\n",
    "print(\"----------------\")\n",
    "print(f\"lr_model.linear.weight: {lr_model.linear.weight}\")\n",
    "print(f\"lr_model.linear.bias: {lr_model.linear.bias}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
